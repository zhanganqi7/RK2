{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7175fd29",
   "metadata": {},
   "source": [
    "# Рубежный контроль №2\n",
    "# Группа: ИУ5И-22М\n",
    "# Выполнила: Чжан Аньци\n",
    "# Задание:Решение задачи классификации текстов.\n",
    "Необходимо решить задачу классификации текстов, сформировав два варианта векторизации признаков - на основе CountVectorizer и на основе TfidfVectorizer. В качестве классификаторов необходимо использовать два классификатора:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f42782",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4a55a",
   "metadata": {},
   "source": [
    "Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484dc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8180d2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hasImage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>print they should pay all the back all the mon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why did attorney general loretta lynch plead t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>red state  \\nfox news sunday reported this mor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>email kayla mueller was a prisoner and torture...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>email healthcare reform to make america great ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>freitag  november  gaagnagna zum babywort des ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>freitag  november  junge  in brunnen gefallen ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>samstag  november  bremen ersetzt als erstes b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>morgen in pams  steinmeier mit absoluter mehrh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>sonntag  november  stiftung warentest benotet ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  hasImage\n",
       "0    print they should pay all the back all the mon...         1\n",
       "1    why did attorney general loretta lynch plead t...         1\n",
       "2    red state  \\nfox news sunday reported this mor...         1\n",
       "3    email kayla mueller was a prisoner and torture...         1\n",
       "4    email healthcare reform to make america great ...         1\n",
       "..                                                 ...       ...\n",
       "995  freitag  november  gaagnagna zum babywort des ...         1\n",
       "996  freitag  november  junge  in brunnen gefallen ...         1\n",
       "997  samstag  november  bremen ersetzt als erstes b...         1\n",
       "998  morgen in pams  steinmeier mit absoluter mehrh...         1\n",
       "999  sonntag  november  stiftung warentest benotet ...         1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('news_articles.csv', usecols=['text', 'hasImage'],nrows=1000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ef2fa",
   "metadata": {},
   "source": [
    "# Предобработка признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9bdff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x29611 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 197165 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfv = TfidfVectorizer()\n",
    "tfidf_ngram_features = tfidfv.fit_transform(df['text'])\n",
    "tfidf_ngram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f894ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x29611 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 197165 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec = CountVectorizer()\n",
    "countvec_ngram_features = countvec.fit_transform(df['text'])\n",
    "countvec_ngram_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326ec7a",
   "metadata": {},
   "source": [
    "# Random Forest Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c58105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9444    0.4857    0.6415        70\n",
      "           0     0.8636    0.9913    0.9231       230\n",
      "\n",
      "    accuracy                         0.8733       300\n",
      "   macro avg     0.9040    0.7385    0.7823       300\n",
      "weighted avg     0.8825    0.8733    0.8574       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFIDF + RFC\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_ngram_features, df['hasImage'], test_size=0.3, random_state=1)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4, target_names=list(map(str, list(y_test.unique())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e9ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.8750    0.6000    0.7119        70\n",
      "           0     0.8889    0.9739    0.9295       230\n",
      "\n",
      "    accuracy                         0.8867       300\n",
      "   macro avg     0.8819    0.7870    0.8207       300\n",
      "weighted avg     0.8856    0.8867    0.8787       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVec + RFC\n",
    "X_train, X_test, y_train, y_test = train_test_split(countvec_ngram_features, df['hasImage'], test_size=0.3, random_state=1)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4, target_names=list(map(str, list(y_test.unique())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1665a0",
   "metadata": {},
   "source": [
    "# Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d65112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     1.0000    0.0286    0.0556        70\n",
      "           0     0.7718    1.0000    0.8712       230\n",
      "\n",
      "    accuracy                         0.7733       300\n",
      "   macro avg     0.8859    0.5143    0.4634       300\n",
      "weighted avg     0.8251    0.7733    0.6809       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFIDF + CNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_ngram_features, df['hasImage'], test_size=0.3, random_state=1)\n",
    "model = ComplementNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4, target_names=list(map(str, list(y_test.unique())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc39a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.8846    0.3286    0.4792        70\n",
      "           0     0.8285    0.9870    0.9008       230\n",
      "\n",
      "    accuracy                         0.8333       300\n",
      "   macro avg     0.8565    0.6578    0.6900       300\n",
      "weighted avg     0.8416    0.8333    0.8024       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVec + CNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(countvec_ngram_features, df['hasImage'], test_size=0.3, random_state=1)\n",
    "model = ComplementNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4, target_names=list(map(str, list(y_test.unique())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012ccc5",
   "metadata": {},
   "source": [
    "# Выводы:\n",
    "1. CountVectorizer показал лучший результат в обоих моделях\n",
    "2.Random Forest Classifier показал лучшие, чем Complement Naive Bayes результаты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
